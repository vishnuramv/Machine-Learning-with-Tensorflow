{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPM9I+XjRnVYScNdntnOFq+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QhxIEXsJy08",
        "colab_type": "text"
      },
      "source": [
        "#NLP with CNN,LSTM,GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE3cNLo5KDP7",
        "colab_type": "text"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0KE5jhEKHtE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngTkC8ecKJUt",
        "colab_type": "text"
      },
      "source": [
        "##Get Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qly_Bk1LKOVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the dataset.\n",
        "# It has 70000 items, so might take a while to download\n",
        "dataset, info = tfds.load('glue/sst2', with_info=True)\n",
        "print(info.features)\n",
        "print(info.features[\"label\"].num_classes)\n",
        "print(info.features[\"label\"].names)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPRnp80cKRgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the training and validation datasets\n",
        "dataset_train, dataset_validation = dataset['train'], dataset['validation']\n",
        "dataset_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-3UmVnEKTAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print some of the entries\n",
        "for example in dataset_train.take(2):  \n",
        "  review, label = example[\"sentence\"], example[\"label\"]\n",
        "  print(\"Review:\", review)\n",
        "  print(\"Label: %d \\n\" % label.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySFT5JYOKWbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the sentences and the labels\n",
        "# for both the training and the validation sets\n",
        "training_reviews = []\n",
        "training_labels = []\n",
        " \n",
        "validation_reviews = []\n",
        "validation_labels = []\n",
        "\n",
        "# The dataset has 67,000 training entries, but that's a lot to process here!\n",
        "\n",
        "# If you want to take the entire dataset: WARNING: takes longer!!\n",
        "# for item in dataset_train.take(-1):\n",
        "\n",
        "# Take 10,000 reviews\n",
        "for item in dataset_train.take(40000):\n",
        "  review, label = item[\"sentence\"], item[\"label\"]\n",
        "  training_reviews.append(str(review.numpy()))\n",
        "  training_labels.append(label.numpy())\n",
        "\n",
        "print (\"\\nNumber of training reviews is: \", len(training_reviews))\n",
        "\n",
        "# print some of the reviews and labels\n",
        "for i in range(0, 2):\n",
        "  print (training_reviews[i])\n",
        "  print (training_labels[i])\n",
        "\n",
        "# Get the validation data\n",
        "# there's only about 800 items, so take them all\n",
        "for item in dataset_validation.take(-1):  \n",
        "  review, label = item[\"sentence\"], item[\"label\"]\n",
        "  validation_reviews.append(str(review.numpy()))\n",
        "  validation_labels.append(label.numpy())\n",
        "\n",
        "print (\"\\nNumber of validation reviews is: \", len(validation_reviews))\n",
        "\n",
        "# Print some of the validation reviews and labels\n",
        "for i in range(0, 2):\n",
        "  print (validation_reviews[i])\n",
        "  print (validation_labels[i])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "276t6UzzKcVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# There's a total of 21224 words in the reviews\n",
        "# but many of them are irrelevant like with, it, of, on.\n",
        "# If we take a subset of the training data, then the vocab\n",
        "# will be smaller.\n",
        "\n",
        "# A reasonable review might have about 50 words or so,\n",
        "# so we can set max_length to 50 (but feel free to change it as you like)\n",
        "\n",
        "vocab_size = 4000\n",
        "embedding_dim = 16\n",
        "max_length = 50\n",
        "trunc_type='post'\n",
        "pad_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_reviews)\n",
        "word_index = tokenizer.word_index\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoN1nIgjKdyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad the sequences so that they are all the same length\n",
        "training_sequences = tokenizer.texts_to_sequences(training_reviews)\n",
        "training_padded = pad_sequences(training_sequences,maxlen=max_length, \n",
        "                                truncating=trunc_type, padding=pad_type)\n",
        "\n",
        "validation_sequences = tokenizer.texts_to_sequences(validation_reviews)\n",
        "validation_padded = pad_sequences(validation_sequences,maxlen=max_length)\n",
        "\n",
        "training_labels_final = np.array(training_labels)\n",
        "validation_labels_final = np.array(validation_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv9U_mGBKfWz",
        "colab_type": "text"
      },
      "source": [
        "##Creating Embedding Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaltFeUbKkEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),  \n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKqje4sDKn4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 20\n",
        "history = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n",
        "                    validation_data=(validation_padded, validation_labels_final))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPZcCqmWKpyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6PXVHrEKsDf",
        "colab_type": "text"
      },
      "source": [
        "##Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUigDSFpKttW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write some new reviews \n",
        "\n",
        "review1 = \"\"\"I loved this movie\"\"\"\n",
        "\n",
        "review2 = \"\"\"that was the worst movie I've ever seen\"\"\"\n",
        "\n",
        "review3 = \"\"\"too much violence even for a Bond film\"\"\"\n",
        "\n",
        "review4 = \"\"\"a captivating recounting of a cherished myth\"\"\"\n",
        "\n",
        "new_reviews = [review1, review2, review3, review4]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo3IpdawKwUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a function to prepare the new reviews for use with a model\n",
        "# and then use the model to predict the sentiment of the new reviews           \n",
        "\n",
        "def predict_review(model, reviews):\n",
        "  # Create the sequences\n",
        "  padding_type='post'\n",
        "  sample_sequences = tokenizer.texts_to_sequences(reviews)\n",
        "  reviews_padded = pad_sequences(sample_sequences, padding=padding_type, \n",
        "                                 maxlen=max_length) \n",
        "  classes = model.predict(reviews_padded)\n",
        "  for x in range(len(reviews_padded)):\n",
        "    print(reviews[x])\n",
        "    print(classes[x])\n",
        "    print('\\n')\n",
        "\n",
        "predict_review(model, new_reviews)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQYMbcfrKzme",
        "colab_type": "text"
      },
      "source": [
        "##Define a function to train and show the results of models with different layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n5bXRpTK1nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_model_and_show_results (model, reviews):\n",
        "  model.summary()\n",
        "  history = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n",
        "                      validation_data=(validation_padded, validation_labels_final))\n",
        "  plot_graphs(history, \"accuracy\")\n",
        "  plot_graphs(history, \"loss\")\n",
        "  predict_review(model, reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlK_gPp5K3ga",
        "colab_type": "text"
      },
      "source": [
        "##Using CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8Ru0KUiK5rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 30\n",
        "\n",
        "model_cnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Conv1D(16, 5, activation='relu'),\n",
        "    tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Default learning rate for the Adam optimizer is 0.001\n",
        "# Let's slow down the learning rate by 10.\n",
        "learning_rate = 0.0001\n",
        "model_cnn.compile(loss='binary_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate), \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "fit_model_and_show_results(model_cnn, new_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjETeeRuLA00",
        "colab_type": "text"
      },
      "source": [
        "##Using GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adXUFVwVK9x8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 30\n",
        "\n",
        "model_gru = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "learning_rate = 0.00003 # slower than the default learning rate\n",
        "model_gru.compile(loss='binary_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "fit_model_and_show_results(model_gru, new_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPC7irVZLDTk",
        "colab_type": "text"
      },
      "source": [
        "##Using LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqCSsnc5LF_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 30\n",
        "\n",
        "model_bidi_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)), \n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "learning_rate = 0.00003\n",
        "model_bidi_lstm.compile(loss='binary_crossentropy',\n",
        "                        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                        metrics=['accuracy'])\n",
        "fit_model_and_show_results(model_bidi_lstm, new_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AR62X7vLIKn",
        "colab_type": "text"
      },
      "source": [
        "##Using Multiple LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4oYc5dALK03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 30\n",
        "\n",
        "model_multiple_bidi_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim, \n",
        "                                                       return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "learning_rate = 0.0003\n",
        "model_multiple_bidi_lstm.compile(loss='binary_crossentropy',\n",
        "                                 optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                                 metrics=['accuracy'])\n",
        "fit_model_and_show_results(model_multiple_bidi_lstm, new_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxy13mqALPcR",
        "colab_type": "text"
      },
      "source": [
        "##Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOP_d-qZLTA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write some new reviews \n",
        "\n",
        "review1 = \"\"\"I loved this movie\"\"\"\n",
        "\n",
        "review2 = \"\"\"that was the worst movie I've ever seen\"\"\"\n",
        "\n",
        "review3 = \"\"\"too much violence even for a Bond film\"\"\"\n",
        "\n",
        "review4 = \"\"\"a captivating recounting of a cherished myth\"\"\"\n",
        "\n",
        "review5 = \"\"\"I saw this movie yesterday and I was feeling low to start with,\n",
        " but it was such a wonderful movie that it lifted my spirits and brightened \n",
        " my day, you can\\'t go wrong with a movie with Whoopi Goldberg in it.\"\"\"\n",
        "\n",
        "review6 = \"\"\"I don\\'t understand why it received an oscar recommendation\n",
        " for best movie, it was long and boring\"\"\"\n",
        "\n",
        "review7 = \"\"\"the scenery was magnificent, the CGI of the dogs was so realistic I\n",
        " thought they were played by real dogs even though they talked!\"\"\"\n",
        "\n",
        "review8 = \"\"\"The ending was so sad and yet so uplifting at the same time. \n",
        " I'm looking for an excuse to see it again\"\"\"\n",
        "\n",
        "review9 = \"\"\"I had expected so much more from a movie made by the director \n",
        " who made my most favorite movie ever, I was very disappointed in the tedious \n",
        " story\"\"\"\n",
        "\n",
        "review10 = \"I wish I could watch this movie every day for the rest of my life\"\n",
        "\n",
        "more_reviews = [review1, review2, review3, review4, review5, review6, review7, \n",
        "               review8, review9, review10]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEYrVkcVLiEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"============================\\n\",\"Embeddings only:\\n\", \"============================\")\n",
        "predict_review(model, more_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFqlwGM-LkAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"============================\\n\",\"With CNN\\n\", \"============================\")\n",
        "predict_review(model_cnn, more_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP-HgUeOLl2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"===========================\\n\",\"With bidirectional GRU\\n\", \"============================\")\n",
        "predict_review(model_gru, more_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l291XEL9Lo5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"===========================\\n\", \"With a single bidirectional LSTM:\\n\", \"===========================\")\n",
        "predict_review(model_bidi_lstm, more_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ0ocxHDLrF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"===========================\\n\", \"With multiple bidirectional LSTM:\\n\", \"==========================\")\n",
        "predict_review(model_multiple_bidi_lstm, more_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}